{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-28T17:14:33.708587Z",
     "start_time": "2025-03-28T17:14:18.100378Z"
    }
   },
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from build_graph import BuildGraph\n",
    "from py_hgat import GAT\n",
    "import torch.nn.functional as F\n",
    "import torch \n",
    "import time\n",
    "from torcheval.metrics.functional import multiclass_f1_score\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T17:14:46.727777Z",
     "start_time": "2025-03-28T17:14:33.708587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Build Graph\n",
    "g = BuildGraph(\"UIT_VFSC\").g\n",
    "print(g)"
   ],
   "id": "382fe12cc2839595",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step pre processing\n",
      "step add word doc edge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 16175it [00:02, 6311.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step add word word edge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Constructing word pair count frequency: 100%|██████████| 87309/87309 [00:03<00:00, 21977.41it/s]\n",
      "Adding word_word edges: 100%|██████████| 337534/337534 [00:00<00:00, 520706.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step setup graph\n",
      "Graph(num_nodes=19020, num_edges=595385,\n",
      "      ndata_schemes={'x': Scheme(shape=(2845,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.float32), 'train_mask': Scheme(shape=(), dtype=torch.bool), 'test_mask': Scheme(shape=(), dtype=torch.bool)}\n",
      "      edata_schemes={'weight': Scheme(shape=(), dtype=torch.float32)})\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T17:14:46.749914Z",
     "start_time": "2025-03-28T17:14:46.729780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "node_features = g.ndata['x']\n",
    "node_labels = g.ndata['label']\n",
    "train_mask = g.ndata['train_mask']\n",
    "test_mask = g.ndata['test_mask']\n",
    "n_features = node_features.shape[1]\n",
    "n_labels = int(node_labels.max().item() + 1)\n",
    "print(n_features)\n",
    "print(node_features.shape)"
   ],
   "id": "588f24fac1ecb8a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2845\n",
      "torch.Size([19020, 2845])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T17:14:46.762683Z",
     "start_time": "2025-03-28T17:14:46.752423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(model, graph, features, labels, mask, edge_weights=None):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        logits = model(graph, features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "\n",
    "        correct = torch.sum(indices == labels)\n",
    "\n",
    "        acc = correct.item() * 1.0 / len(labels)\n",
    "        mf1 = multiclass_f1_score(indices.type(torch.long), labels.type(torch.long), num_classes=n_labels, average='macro')\n",
    "        wf1 = multiclass_f1_score(indices.type(torch.long), labels.type(torch.long), num_classes=n_labels, average='weighted')\n",
    "\n",
    "        return acc, mf1, wf1"
   ],
   "id": "18f98486a204e57a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T17:14:46.831049Z",
     "start_time": "2025-03-28T17:14:46.766211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = GAT(\n",
    "                 num_layers=1,\n",
    "                 in_dim=n_features,\n",
    "                 num_hidden=200,\n",
    "                 num_classes=n_labels,\n",
    "                 heads=[4] + [1],\n",
    "                 activation=F.elu,\n",
    "                 feat_drop=0.5,\n",
    "                 attn_drop=0.5,\n",
    "        )\n",
    "opt = torch.optim.Adam(model.parameters(), lr=5e-3)"
   ],
   "id": "69f77c3fca2f8bcd",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-28T17:14:46.831049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dur = []\n",
    "max_acc = 0\n",
    "max_f1 = 0\n",
    "for epoch in range(1000):\n",
    "    if epoch >= 3:\n",
    "        t0 = time.time()\n",
    "    model.train()\n",
    "    # forward propagation by using all nodes\n",
    "    logits = model(g, node_features)\n",
    "    \n",
    "    # compute loss\n",
    "    loss = F.cross_entropy(logits[train_mask].to(torch.float32), node_labels[train_mask].to(torch.long))\n",
    "\n",
    "    # backward propagation\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if epoch >= 3:\n",
    "        dur.append(time.time() - t0)\n",
    "\n",
    "    # compute validation accuracy\n",
    "    acc, mf1, wf1 = evaluate(model, g, node_features, node_labels, test_mask)\n",
    "    print(f\"Epoch {epoch:05d} | Loss {loss.item():.4f} | Test Acc {acc:.4f} | mF1 {mf1:.4f} | wF1 {wf1:.4f} | Time(s) {np.mean(dur):.4f}\")\n",
    "\n",
    "    if acc > max_acc:\n",
    "        max_acc = acc\n",
    "    max_f1 = max(max(max_f1, wf1), mf1)"
   ],
   "id": "8f5ae83c5b842335",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Loss 1.1054 | Test Acc 0.5816 | mF1 0.3367 | wF1 0.4916 | Time(s) nan\n",
      "Epoch 00001 | Loss 0.8827 | Test Acc 0.7593 | mF1 0.5163 | wF1 0.7374 | Time(s) nan\n",
      "Epoch 00002 | Loss 0.7825 | Test Acc 0.7707 | mF1 0.5273 | wF1 0.7516 | Time(s) nan\n",
      "Epoch 00003 | Loss 0.7426 | Test Acc 0.7597 | mF1 0.5198 | wF1 0.7407 | Time(s) 4.3610\n",
      "Epoch 00004 | Loss 0.7368 | Test Acc 0.7543 | mF1 0.5159 | wF1 0.7356 | Time(s) 4.2659\n",
      "Epoch 00005 | Loss 0.7412 | Test Acc 0.7446 | mF1 0.5093 | wF1 0.7254 | Time(s) 4.3451\n",
      "Epoch 00006 | Loss 0.7248 | Test Acc 0.7326 | mF1 0.5004 | wF1 0.7122 | Time(s) 4.9874\n",
      "Epoch 00007 | Loss 0.7411 | Test Acc 0.7494 | mF1 0.5128 | wF1 0.7305 | Time(s) 5.4236\n",
      "Epoch 00008 | Loss 0.7313 | Test Acc 0.7614 | mF1 0.5210 | wF1 0.7425 | Time(s) 5.2635\n",
      "Epoch 00009 | Loss 0.7171 | Test Acc 0.7635 | mF1 0.5224 | wF1 0.7446 | Time(s) 5.2834\n",
      "Epoch 00010 | Loss 0.7260 | Test Acc 0.7501 | mF1 0.5131 | wF1 0.7309 | Time(s) 5.2956\n",
      "Epoch 00011 | Loss 0.6967 | Test Acc 0.7543 | mF1 0.5160 | wF1 0.7349 | Time(s) 5.2729\n",
      "Epoch 00012 | Loss 0.6849 | Test Acc 0.7675 | mF1 0.5252 | wF1 0.7485 | Time(s) 5.1393\n",
      "Epoch 00013 | Loss 0.6694 | Test Acc 0.7797 | mF1 0.5334 | wF1 0.7604 | Time(s) 5.0508\n",
      "Epoch 00014 | Loss 0.6573 | Test Acc 0.7880 | mF1 0.5389 | wF1 0.7683 | Time(s) 4.9465\n",
      "Epoch 00015 | Loss 0.6613 | Test Acc 0.7877 | mF1 0.5390 | wF1 0.7682 | Time(s) 4.8579\n",
      "Epoch 00016 | Loss 0.6459 | Test Acc 0.7736 | mF1 0.5293 | wF1 0.7541 | Time(s) 4.8402\n",
      "Epoch 00017 | Loss 0.6575 | Test Acc 0.7646 | mF1 0.5229 | wF1 0.7446 | Time(s) 4.9179\n",
      "Epoch 00018 | Loss 0.6664 | Test Acc 0.7856 | mF1 0.5376 | wF1 0.7660 | Time(s) 5.1113\n",
      "Epoch 00019 | Loss 0.6427 | Test Acc 0.8037 | mF1 0.5499 | wF1 0.7839 | Time(s) 5.0656\n",
      "Epoch 00020 | Loss 0.6458 | Test Acc 0.8124 | mF1 0.5555 | wF1 0.7920 | Time(s) 5.0805\n",
      "Epoch 00021 | Loss 0.6454 | Test Acc 0.8139 | mF1 0.5564 | wF1 0.7934 | Time(s) 5.0449\n",
      "Epoch 00022 | Loss 0.6428 | Test Acc 0.8132 | mF1 0.5562 | wF1 0.7930 | Time(s) 5.0512\n",
      "Epoch 00023 | Loss 0.6295 | Test Acc 0.8069 | mF1 0.5521 | wF1 0.7870 | Time(s) 5.1320\n",
      "Epoch 00024 | Loss 0.6219 | Test Acc 0.8040 | mF1 0.5502 | wF1 0.7841 | Time(s) 5.0951\n",
      "Epoch 00025 | Loss 0.6333 | Test Acc 0.8101 | mF1 0.5543 | wF1 0.7901 | Time(s) 5.1187\n",
      "Epoch 00026 | Loss 0.6184 | Test Acc 0.8164 | mF1 0.5583 | wF1 0.7960 | Time(s) 5.1418\n",
      "Epoch 00027 | Loss 0.6078 | Test Acc 0.8170 | mF1 0.5585 | wF1 0.7965 | Time(s) 5.1868\n",
      "Epoch 00028 | Loss 0.6076 | Test Acc 0.8181 | mF1 0.5593 | wF1 0.7975 | Time(s) 5.1982\n",
      "Epoch 00029 | Loss 0.6042 | Test Acc 0.8172 | mF1 0.5590 | wF1 0.7970 | Time(s) 5.1802\n",
      "Epoch 00030 | Loss 0.6022 | Test Acc 0.8147 | mF1 0.5574 | wF1 0.7947 | Time(s) 5.2628\n",
      "Epoch 00031 | Loss 0.6111 | Test Acc 0.8191 | mF1 0.5603 | wF1 0.7988 | Time(s) 5.2915\n",
      "Epoch 00032 | Loss 0.5889 | Test Acc 0.8227 | mF1 0.5624 | wF1 0.8019 | Time(s) 5.3501\n",
      "Epoch 00033 | Loss 0.5911 | Test Acc 0.8246 | mF1 0.5636 | wF1 0.8036 | Time(s) 5.4565\n",
      "Epoch 00034 | Loss 0.5901 | Test Acc 0.8280 | mF1 0.5661 | wF1 0.8071 | Time(s) 5.9470\n",
      "Epoch 00035 | Loss 0.5921 | Test Acc 0.8288 | mF1 0.5668 | wF1 0.8081 | Time(s) 6.7024\n",
      "Epoch 00036 | Loss 0.5763 | Test Acc 0.8322 | mF1 0.5690 | wF1 0.8112 | Time(s) 7.0169\n",
      "Epoch 00037 | Loss 0.5704 | Test Acc 0.8366 | mF1 0.5746 | wF1 0.8157 | Time(s) 7.1175\n",
      "Epoch 00038 | Loss 0.5666 | Test Acc 0.8383 | mF1 0.5757 | wF1 0.8173 | Time(s) 7.2755\n",
      "Epoch 00039 | Loss 0.5691 | Test Acc 0.8398 | mF1 0.5766 | wF1 0.8186 | Time(s) 7.3178\n",
      "Epoch 00040 | Loss 0.5731 | Test Acc 0.8433 | mF1 0.5792 | wF1 0.8222 | Time(s) 7.2670\n",
      "Epoch 00041 | Loss 0.5600 | Test Acc 0.8438 | mF1 0.5796 | wF1 0.8228 | Time(s) 7.1968\n",
      "Epoch 00042 | Loss 0.5532 | Test Acc 0.8393 | mF1 0.5762 | wF1 0.8181 | Time(s) 7.1297\n",
      "Epoch 00043 | Loss 0.5543 | Test Acc 0.8301 | mF1 0.5693 | wF1 0.8084 | Time(s) 7.0706\n",
      "Epoch 00044 | Loss 0.5633 | Test Acc 0.8374 | mF1 0.5746 | wF1 0.8159 | Time(s) 7.0602\n",
      "Epoch 00045 | Loss 0.5495 | Test Acc 0.8433 | mF1 0.5790 | wF1 0.8220 | Time(s) 7.0529\n",
      "Epoch 00046 | Loss 0.5417 | Test Acc 0.8480 | mF1 0.5823 | wF1 0.8267 | Time(s) 7.0289\n",
      "Epoch 00047 | Loss 0.5455 | Test Acc 0.8497 | mF1 0.5834 | wF1 0.8283 | Time(s) 7.0635\n",
      "Epoch 00048 | Loss 0.5484 | Test Acc 0.8429 | mF1 0.5786 | wF1 0.8214 | Time(s) 7.0348\n",
      "Epoch 00049 | Loss 0.5338 | Test Acc 0.8391 | mF1 0.5758 | wF1 0.8174 | Time(s) 7.0000\n",
      "Epoch 00050 | Loss 0.5377 | Test Acc 0.8446 | mF1 0.5797 | wF1 0.8230 | Time(s) 6.9541\n",
      "Epoch 00051 | Loss 0.5385 | Test Acc 0.8545 | mF1 0.5868 | wF1 0.8331 | Time(s) 6.9375\n",
      "Epoch 00052 | Loss 0.5343 | Test Acc 0.8564 | mF1 0.5883 | wF1 0.8351 | Time(s) 6.8850\n",
      "Epoch 00053 | Loss 0.5375 | Test Acc 0.8551 | mF1 0.5871 | wF1 0.8335 | Time(s) 6.8735\n",
      "Epoch 00054 | Loss 0.5302 | Test Acc 0.8435 | mF1 0.5787 | wF1 0.8216 | Time(s) 6.8722\n",
      "Epoch 00055 | Loss 0.5165 | Test Acc 0.8318 | mF1 0.5702 | wF1 0.8096 | Time(s) 6.8598\n",
      "Epoch 00056 | Loss 0.5173 | Test Acc 0.8381 | mF1 0.5748 | wF1 0.8160 | Time(s) 6.8658\n",
      "Epoch 00057 | Loss 0.5244 | Test Acc 0.8549 | mF1 0.5869 | wF1 0.8331 | Time(s) 6.8523\n",
      "Epoch 00058 | Loss 0.5243 | Test Acc 0.8621 | mF1 0.5922 | wF1 0.8406 | Time(s) 6.8542\n",
      "Epoch 00059 | Loss 0.5200 | Test Acc 0.8629 | mF1 0.5927 | wF1 0.8413 | Time(s) 6.8440\n",
      "Epoch 00060 | Loss 0.5335 | Test Acc 0.8467 | mF1 0.5809 | wF1 0.8246 | Time(s) 6.8368\n",
      "Epoch 00061 | Loss 0.5116 | Test Acc 0.8155 | mF1 0.5584 | wF1 0.7930 | Time(s) 6.8130\n",
      "Epoch 00062 | Loss 0.5469 | Test Acc 0.8288 | mF1 0.5681 | wF1 0.8065 | Time(s) 6.8045\n",
      "Epoch 00063 | Loss 0.5749 | Test Acc 0.8625 | mF1 0.6001 | wF1 0.8416 | Time(s) 6.7682\n",
      "Epoch 00064 | Loss 0.5089 | Test Acc 0.8600 | mF1 0.6014 | wF1 0.8400 | Time(s) 6.7360\n",
      "Epoch 00065 | Loss 0.5516 | Test Acc 0.8612 | mF1 0.6023 | wF1 0.8412 | Time(s) 6.7337\n",
      "Epoch 00066 | Loss 0.5245 | Test Acc 0.8663 | mF1 0.6055 | wF1 0.8459 | Time(s) 6.7070\n",
      "Epoch 00067 | Loss 0.5258 | Test Acc 0.8457 | mF1 0.5906 | wF1 0.8248 | Time(s) 6.6692\n",
      "Epoch 00068 | Loss 0.4914 | Test Acc 0.8084 | mF1 0.5608 | wF1 0.7863 | Time(s) 6.6306\n",
      "Epoch 00069 | Loss 0.5157 | Test Acc 0.7995 | mF1 0.5541 | wF1 0.7770 | Time(s) 6.5962\n",
      "Epoch 00070 | Loss 0.5424 | Test Acc 0.8324 | mF1 0.5785 | wF1 0.8110 | Time(s) 6.5650\n",
      "Epoch 00071 | Loss 0.5174 | Test Acc 0.8640 | mF1 0.6037 | wF1 0.8432 | Time(s) 6.5487\n",
      "Epoch 00072 | Loss 0.5038 | Test Acc 0.8654 | mF1 0.6051 | wF1 0.8452 | Time(s) 6.5549\n",
      "Epoch 00073 | Loss 0.5327 | Test Acc 0.8642 | mF1 0.6043 | wF1 0.8440 | Time(s) 6.5235\n",
      "Epoch 00074 | Loss 0.5440 | Test Acc 0.8699 | mF1 0.6079 | wF1 0.8492 | Time(s) 6.4882\n",
      "Epoch 00075 | Loss 0.4862 | Test Acc 0.8520 | mF1 0.5925 | wF1 0.8309 | Time(s) 6.4830\n",
      "Epoch 00076 | Loss 0.4762 | Test Acc 0.8206 | mF1 0.5698 | wF1 0.7989 | Time(s) 6.4490\n",
      "Epoch 00077 | Loss 0.5053 | Test Acc 0.8086 | mF1 0.5608 | wF1 0.7864 | Time(s) 6.4194\n",
      "Epoch 00078 | Loss 0.4990 | Test Acc 0.8240 | mF1 0.5722 | wF1 0.8023 | Time(s) 6.3858\n",
      "Epoch 00079 | Loss 0.4967 | Test Acc 0.8461 | mF1 0.5883 | wF1 0.8249 | Time(s) 6.3557\n",
      "Epoch 00080 | Loss 0.4934 | Test Acc 0.8652 | mF1 0.6020 | wF1 0.8443 | Time(s) 6.3391\n",
      "Epoch 00081 | Loss 0.4865 | Test Acc 0.8694 | mF1 0.6077 | wF1 0.8489 | Time(s) 6.3269\n",
      "Epoch 00082 | Loss 0.4989 | Test Acc 0.8688 | mF1 0.6046 | wF1 0.8480 | Time(s) 6.3066\n",
      "Epoch 00083 | Loss 0.5040 | Test Acc 0.8636 | mF1 0.6008 | wF1 0.8426 | Time(s) 6.2814\n",
      "Epoch 00084 | Loss 0.5024 | Test Acc 0.8530 | mF1 0.5932 | wF1 0.8319 | Time(s) 6.2712\n",
      "Epoch 00085 | Loss 0.4674 | Test Acc 0.8322 | mF1 0.5781 | wF1 0.8107 | Time(s) 6.2676\n",
      "Epoch 00086 | Loss 0.4868 | Test Acc 0.8208 | mF1 0.5698 | wF1 0.7989 | Time(s) 6.2501\n",
      "Epoch 00087 | Loss 0.4765 | Test Acc 0.8170 | mF1 0.5670 | wF1 0.7950 | Time(s) 6.2300\n",
      "Epoch 00088 | Loss 0.4968 | Test Acc 0.8315 | mF1 0.5777 | wF1 0.8100 | Time(s) 6.2080\n",
      "Epoch 00089 | Loss 0.4781 | Test Acc 0.8524 | mF1 0.5927 | wF1 0.8311 | Time(s) 6.1909\n",
      "Epoch 00090 | Loss 0.4620 | Test Acc 0.8619 | mF1 0.5995 | wF1 0.8407 | Time(s) 6.1862\n",
      "Epoch 00091 | Loss 0.4643 | Test Acc 0.8636 | mF1 0.6008 | wF1 0.8425 | Time(s) 6.1672\n",
      "Epoch 00092 | Loss 0.4734 | Test Acc 0.8652 | mF1 0.6045 | wF1 0.8444 | Time(s) 6.1446\n",
      "Epoch 00093 | Loss 0.4749 | Test Acc 0.8593 | mF1 0.5977 | wF1 0.8381 | Time(s) 6.1265\n",
      "Epoch 00094 | Loss 0.4574 | Test Acc 0.8374 | mF1 0.5820 | wF1 0.8161 | Time(s) 6.1088\n",
      "Epoch 00095 | Loss 0.4746 | Test Acc 0.8263 | mF1 0.5738 | wF1 0.8045 | Time(s) 6.0940\n",
      "Epoch 00096 | Loss 0.4851 | Test Acc 0.8280 | mF1 0.5751 | wF1 0.8063 | Time(s) 6.0833\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 16\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# backward propagation\u001B[39;00m\n\u001B[0;32m     15\u001B[0m opt\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 16\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m opt\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m epoch \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m3\u001B[39m:\n",
      "File \u001B[1;32mD:\\nguyennha\\TextGraph\\.venv\\Lib\\site-packages\\torch\\_tensor.py:525\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    515\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    517\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    518\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    523\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    524\u001B[0m     )\n\u001B[1;32m--> 525\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    526\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    527\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\nguyennha\\TextGraph\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    262\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    264\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    265\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    266\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 267\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    274\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    275\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\nguyennha\\TextGraph\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    742\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    743\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 744\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    745\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    746\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    747\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    748\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "File \u001B[1;32mD:\\nguyennha\\TextGraph\\.venv\\Lib\\site-packages\\torch\\autograd\\function.py:301\u001B[0m, in \u001B[0;36mBackwardCFunction.apply\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m    295\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    296\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mImplementing both \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbackward\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m and \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvjp\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m for a custom \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    297\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFunction is not allowed. You should only implement one \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    298\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mof them.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    299\u001B[0m     )\n\u001B[0;32m    300\u001B[0m user_fn \u001B[38;5;241m=\u001B[39m vjp_fn \u001B[38;5;28;01mif\u001B[39;00m vjp_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m Function\u001B[38;5;241m.\u001B[39mvjp \u001B[38;5;28;01melse\u001B[39;00m backward_fn\n\u001B[1;32m--> 301\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43muser_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\nguyennha\\TextGraph\\.venv\\Lib\\site-packages\\dgl\\backend\\pytorch\\sparse.py:746\u001B[0m, in \u001B[0;36mEdgeSoftmax.backward\u001B[1;34m(ctx, grad_out)\u001B[0m\n\u001B[0;32m    744\u001B[0m     grad_score \u001B[38;5;241m=\u001B[39m sds \u001B[38;5;241m-\u001B[39m gsddmm(gidx, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmul\u001B[39m\u001B[38;5;124m\"\u001B[39m, out, accum, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124me\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    745\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 746\u001B[0m     grad_score \u001B[38;5;241m=\u001B[39m \u001B[43m_edge_softmax_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgidx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    747\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, grad_score, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mD:\\nguyennha\\TextGraph\\.venv\\Lib\\site-packages\\dgl\\_sparse_ops.py:112\u001B[0m, in \u001B[0;36m_edge_softmax_backward\u001B[1;34m(gidx, out, sds)\u001B[0m\n\u001B[0;32m    110\u001B[0m op \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcopy_rhs\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    111\u001B[0m back_out \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mzeros_like(out)\n\u001B[1;32m--> 112\u001B[0m \u001B[43m_CAPI_DGLKernelEdge_softmax_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    113\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgidx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    114\u001B[0m \u001B[43m    \u001B[49m\u001B[43mop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    115\u001B[0m \u001B[43m    \u001B[49m\u001B[43mto_dgl_nd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    116\u001B[0m \u001B[43m    \u001B[49m\u001B[43mto_dgl_nd\u001B[49m\u001B[43m(\u001B[49m\u001B[43msds\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    117\u001B[0m \u001B[43m    \u001B[49m\u001B[43mto_dgl_nd_for_write\u001B[49m\u001B[43m(\u001B[49m\u001B[43mback_out\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    118\u001B[0m \u001B[43m    \u001B[49m\u001B[43mto_dgl_nd\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    119\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m back_out\n",
      "File \u001B[1;32mD:\\nguyennha\\TextGraph\\.venv\\Lib\\site-packages\\dgl\\_ffi\\_ctypes\\function.py:213\u001B[0m, in \u001B[0;36mFunctionBase.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m    210\u001B[0m ret_val \u001B[38;5;241m=\u001B[39m DGLValue()\n\u001B[0;32m    211\u001B[0m ret_tcode \u001B[38;5;241m=\u001B[39m ctypes\u001B[38;5;241m.\u001B[39mc_int()\n\u001B[0;32m    212\u001B[0m check_call(\n\u001B[1;32m--> 213\u001B[0m     \u001B[43m_LIB\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDGLFuncCall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    214\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    215\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    216\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtcodes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    217\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctypes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mc_int\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_args\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    218\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctypes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbyref\u001B[49m\u001B[43m(\u001B[49m\u001B[43mret_val\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    219\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctypes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbyref\u001B[49m\u001B[43m(\u001B[49m\u001B[43mret_tcode\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    220\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    221\u001B[0m )\n\u001B[0;32m    222\u001B[0m _ \u001B[38;5;241m=\u001B[39m temp_args\n\u001B[0;32m    223\u001B[0m _ \u001B[38;5;241m=\u001B[39m args\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T17:28:54.196551Z",
     "start_time": "2025-03-28T17:28:54.186187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Max accuracy: {max_acc:.4f}\")\n",
    "print(f'Max F1: {max_f1:.4f}')"
   ],
   "id": "5b777aad52863995",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max accuracy: 0.8699\n",
      "Max F1: 0.8492\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
