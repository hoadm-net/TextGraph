{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-28T17:08:26.311953Z",
     "start_time": "2025-03-28T17:08:12.151612Z"
    }
   },
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from build_graph import BuildGraph\n",
    "from py_graphsage import SAGE\n",
    "import torch.nn.functional as F\n",
    "import torch \n",
    "import time\n",
    "from torcheval.metrics.functional import multiclass_f1_score\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T17:08:43.278668Z",
     "start_time": "2025-03-28T17:08:26.314131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Build Graph\n",
    "g = BuildGraph(\"UIT_VFSC\").g\n",
    "print(g)"
   ],
   "id": "3e6dc7adcf2403e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step pre processing\n",
      "step add word doc edge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 16175it [00:02, 7458.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step add word word edge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Constructing word pair count frequency: 100%|██████████| 87309/87309 [00:05<00:00, 15725.12it/s]\n",
      "Adding word_word edges: 100%|██████████| 337534/337534 [00:00<00:00, 358506.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step setup graph\n",
      "Graph(num_nodes=19020, num_edges=595385,\n",
      "      ndata_schemes={'x': Scheme(shape=(2845,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.float32), 'train_mask': Scheme(shape=(), dtype=torch.bool), 'test_mask': Scheme(shape=(), dtype=torch.bool)}\n",
      "      edata_schemes={'weight': Scheme(shape=(), dtype=torch.float32)})\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T17:08:43.341152Z",
     "start_time": "2025-03-28T17:08:43.293628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "node_features = g.ndata['x']\n",
    "node_labels = g.ndata['label']\n",
    "train_mask = g.ndata['train_mask']\n",
    "test_mask = g.ndata['test_mask']\n",
    "n_features = node_features.shape[1]\n",
    "n_labels = int(node_labels.max().item() + 1)\n",
    "print(n_features)\n",
    "print(node_features.shape)"
   ],
   "id": "2436ecc98dc89e65",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2845\n",
      "torch.Size([19020, 2845])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T17:08:43.365567Z",
     "start_time": "2025-03-28T17:08:43.343559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# evaluate model\n",
    "def evaluate(model, graph, features, labels, mask, edge_weights=None):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        logits = model(graph, features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "\n",
    "        correct = torch.sum(indices == labels)\n",
    "\n",
    "        acc = correct.item() * 1.0 / len(labels)\n",
    "        mf1 = multiclass_f1_score(indices.type(torch.long), labels.type(torch.long), num_classes=n_labels, average='macro')\n",
    "        wf1 = multiclass_f1_score(indices.type(torch.long), labels.type(torch.long), num_classes=n_labels, average='weighted')\n",
    "        \n",
    "        return acc, mf1, wf1"
   ],
   "id": "6a530886a3aa78b4",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T17:08:43.434918Z",
     "start_time": "2025-03-28T17:08:43.372977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Build model\n",
    "model = SAGE(in_feats=n_features, hid_feats=200, out_feats=n_labels)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=5e-3)"
   ],
   "id": "6c175148037cc85b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T17:13:36.266150Z",
     "start_time": "2025-03-28T17:08:43.437487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# training model\n",
    "dur = []\n",
    "max_acc = 0\n",
    "max_f1 = 0\n",
    "for epoch in range(1000):\n",
    "    if epoch >= 3:\n",
    "        t0 = time.time()\n",
    "    model.train()\n",
    "    # forward propagation by using all nodes\n",
    "    logits = model(g, node_features)\n",
    "    \n",
    "    # compute loss\n",
    "    loss = F.cross_entropy(logits[train_mask].to(torch.float32), node_labels[train_mask].to(torch.long))\n",
    "\n",
    "    # backward propagation\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if epoch >= 3:\n",
    "        dur.append(time.time() - t0)\n",
    "\n",
    "    # compute validation accuracy\n",
    "    acc, mf1, wf1 = evaluate(model, g, node_features, node_labels, test_mask)\n",
    "    print(f\"Epoch {epoch:05d} | Loss {loss.item():.4f} | Test Acc {acc:.4f} | mF1 {mf1:.4f} | wF1 {wf1:.4f} | Time(s) {np.mean(dur):.4f}\")\n",
    "\n",
    "    if acc > max_acc:\n",
    "        max_acc = acc\n",
    "    max_f1 = max(max(max_f1, wf1), mf1)"
   ],
   "id": "2710054248debb2b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Loss 1.0604 | Test Acc 0.7846 | mF1 0.5368 | wF1 0.7648 | Time(s) nan\n",
      "Epoch 00001 | Loss 0.8841 | Test Acc 0.8077 | mF1 0.5527 | wF1 0.7875 | Time(s) nan\n",
      "Epoch 00002 | Loss 0.7501 | Test Acc 0.8170 | mF1 0.5589 | wF1 0.7966 | Time(s) nan\n",
      "Epoch 00003 | Loss 0.6457 | Test Acc 0.8238 | mF1 0.5634 | wF1 0.8031 | Time(s) 1.2052\n",
      "Epoch 00004 | Loss 0.5673 | Test Acc 0.8273 | mF1 0.5657 | wF1 0.8064 | Time(s) 1.2676\n",
      "Epoch 00005 | Loss 0.5107 | Test Acc 0.8318 | mF1 0.5686 | wF1 0.8105 | Time(s) 1.2176\n",
      "Epoch 00006 | Loss 0.4713 | Test Acc 0.8358 | mF1 0.5713 | wF1 0.8143 | Time(s) 1.1989\n",
      "Epoch 00007 | Loss 0.4437 | Test Acc 0.8379 | mF1 0.5727 | wF1 0.8164 | Time(s) 1.1894\n",
      "Epoch 00008 | Loss 0.4227 | Test Acc 0.8410 | mF1 0.5749 | wF1 0.8195 | Time(s) 1.1701\n",
      "Epoch 00009 | Loss 0.4048 | Test Acc 0.8433 | mF1 0.5765 | wF1 0.8218 | Time(s) 1.1880\n",
      "Epoch 00010 | Loss 0.3880 | Test Acc 0.8459 | mF1 0.5783 | wF1 0.8242 | Time(s) 1.1881\n",
      "Epoch 00011 | Loss 0.3709 | Test Acc 0.8492 | mF1 0.5806 | wF1 0.8275 | Time(s) 1.1774\n",
      "Epoch 00012 | Loss 0.3531 | Test Acc 0.8528 | mF1 0.5830 | wF1 0.8309 | Time(s) 1.1523\n",
      "Epoch 00013 | Loss 0.3349 | Test Acc 0.8560 | mF1 0.5852 | wF1 0.8339 | Time(s) 1.1408\n",
      "Epoch 00014 | Loss 0.3170 | Test Acc 0.8600 | mF1 0.5906 | wF1 0.8382 | Time(s) 1.1307\n",
      "Epoch 00015 | Loss 0.3000 | Test Acc 0.8629 | mF1 0.5926 | wF1 0.8411 | Time(s) 1.1320\n",
      "Epoch 00016 | Loss 0.2843 | Test Acc 0.8673 | mF1 0.5957 | wF1 0.8454 | Time(s) 1.1279\n",
      "Epoch 00017 | Loss 0.2705 | Test Acc 0.8709 | mF1 0.6061 | wF1 0.8498 | Time(s) 1.1236\n",
      "Epoch 00018 | Loss 0.2590 | Test Acc 0.8720 | mF1 0.6194 | wF1 0.8524 | Time(s) 1.1094\n",
      "Epoch 00019 | Loss 0.2502 | Test Acc 0.8745 | mF1 0.6331 | wF1 0.8564 | Time(s) 1.0998\n",
      "Epoch 00020 | Loss 0.2437 | Test Acc 0.8753 | mF1 0.6424 | wF1 0.8585 | Time(s) 1.0909\n",
      "Epoch 00021 | Loss 0.2392 | Test Acc 0.8756 | mF1 0.6548 | wF1 0.8604 | Time(s) 1.0809\n",
      "Epoch 00022 | Loss 0.2356 | Test Acc 0.8764 | mF1 0.6724 | wF1 0.8634 | Time(s) 1.0756\n",
      "Epoch 00023 | Loss 0.2318 | Test Acc 0.8787 | mF1 0.6822 | wF1 0.8669 | Time(s) 1.0734\n",
      "Epoch 00024 | Loss 0.2270 | Test Acc 0.8806 | mF1 0.6851 | wF1 0.8690 | Time(s) 1.1241\n",
      "Epoch 00025 | Loss 0.2209 | Test Acc 0.8814 | mF1 0.6855 | wF1 0.8699 | Time(s) 1.1748\n",
      "Epoch 00026 | Loss 0.2138 | Test Acc 0.8819 | mF1 0.6844 | wF1 0.8700 | Time(s) 1.1727\n",
      "Epoch 00027 | Loss 0.2067 | Test Acc 0.8819 | mF1 0.6813 | wF1 0.8695 | Time(s) 1.1676\n",
      "Epoch 00028 | Loss 0.2001 | Test Acc 0.8836 | mF1 0.6797 | wF1 0.8705 | Time(s) 1.1610\n",
      "Epoch 00029 | Loss 0.1944 | Test Acc 0.8840 | mF1 0.6749 | wF1 0.8701 | Time(s) 1.1536\n",
      "Epoch 00030 | Loss 0.1897 | Test Acc 0.8844 | mF1 0.6735 | wF1 0.8702 | Time(s) 1.1498\n",
      "Epoch 00031 | Loss 0.1858 | Test Acc 0.8833 | mF1 0.6732 | wF1 0.8690 | Time(s) 1.1435\n",
      "Epoch 00032 | Loss 0.1824 | Test Acc 0.8829 | mF1 0.6727 | wF1 0.8687 | Time(s) 1.1374\n",
      "Epoch 00033 | Loss 0.1792 | Test Acc 0.8844 | mF1 0.6739 | wF1 0.8701 | Time(s) 1.1307\n",
      "Epoch 00034 | Loss 0.1760 | Test Acc 0.8846 | mF1 0.6737 | wF1 0.8704 | Time(s) 1.1270\n",
      "Epoch 00035 | Loss 0.1726 | Test Acc 0.8844 | mF1 0.6773 | wF1 0.8706 | Time(s) 1.1222\n",
      "Epoch 00036 | Loss 0.1691 | Test Acc 0.8846 | mF1 0.6821 | wF1 0.8718 | Time(s) 1.1163\n",
      "Epoch 00037 | Loss 0.1655 | Test Acc 0.8854 | mF1 0.6881 | wF1 0.8732 | Time(s) 1.1107\n",
      "Epoch 00038 | Loss 0.1618 | Test Acc 0.8850 | mF1 0.6933 | wF1 0.8739 | Time(s) 1.1170\n",
      "Epoch 00039 | Loss 0.1582 | Test Acc 0.8865 | mF1 0.7045 | wF1 0.8763 | Time(s) 1.1128\n",
      "Epoch 00040 | Loss 0.1548 | Test Acc 0.8882 | mF1 0.7068 | wF1 0.8782 | Time(s) 1.1095\n",
      "Epoch 00041 | Loss 0.1518 | Test Acc 0.8869 | mF1 0.7075 | wF1 0.8776 | Time(s) 1.1040\n",
      "Epoch 00042 | Loss 0.1491 | Test Acc 0.8854 | mF1 0.7067 | wF1 0.8766 | Time(s) 1.1014\n",
      "Epoch 00043 | Loss 0.1466 | Test Acc 0.8852 | mF1 0.7087 | wF1 0.8768 | Time(s) 1.0973\n",
      "Epoch 00044 | Loss 0.1443 | Test Acc 0.8854 | mF1 0.7087 | wF1 0.8771 | Time(s) 1.1049\n",
      "Epoch 00045 | Loss 0.1419 | Test Acc 0.8854 | mF1 0.7080 | wF1 0.8772 | Time(s) 1.1055\n",
      "Epoch 00046 | Loss 0.1395 | Test Acc 0.8859 | mF1 0.7115 | wF1 0.8779 | Time(s) 1.1135\n",
      "Epoch 00047 | Loss 0.1369 | Test Acc 0.8861 | mF1 0.7117 | wF1 0.8781 | Time(s) 1.1146\n",
      "Epoch 00048 | Loss 0.1344 | Test Acc 0.8859 | mF1 0.7105 | wF1 0.8776 | Time(s) 1.1180\n",
      "Epoch 00049 | Loss 0.1319 | Test Acc 0.8857 | mF1 0.7113 | wF1 0.8772 | Time(s) 1.1132\n",
      "Epoch 00050 | Loss 0.1295 | Test Acc 0.8859 | mF1 0.7104 | wF1 0.8772 | Time(s) 1.1143\n",
      "Epoch 00051 | Loss 0.1273 | Test Acc 0.8861 | mF1 0.7092 | wF1 0.8772 | Time(s) 1.1117\n",
      "Epoch 00052 | Loss 0.1251 | Test Acc 0.8863 | mF1 0.7096 | wF1 0.8774 | Time(s) 1.1090\n",
      "Epoch 00053 | Loss 0.1231 | Test Acc 0.8861 | mF1 0.7113 | wF1 0.8773 | Time(s) 1.1069\n",
      "Epoch 00054 | Loss 0.1211 | Test Acc 0.8863 | mF1 0.7114 | wF1 0.8775 | Time(s) 1.1114\n",
      "Epoch 00055 | Loss 0.1190 | Test Acc 0.8865 | mF1 0.7132 | wF1 0.8778 | Time(s) 1.1181\n",
      "Epoch 00056 | Loss 0.1170 | Test Acc 0.8867 | mF1 0.7128 | wF1 0.8781 | Time(s) 1.1175\n",
      "Epoch 00057 | Loss 0.1149 | Test Acc 0.8873 | mF1 0.7143 | wF1 0.8790 | Time(s) 1.1170\n",
      "Epoch 00058 | Loss 0.1129 | Test Acc 0.8867 | mF1 0.7174 | wF1 0.8789 | Time(s) 1.1212\n",
      "Epoch 00059 | Loss 0.1110 | Test Acc 0.8869 | mF1 0.7183 | wF1 0.8794 | Time(s) 1.1201\n",
      "Epoch 00060 | Loss 0.1091 | Test Acc 0.8869 | mF1 0.7195 | wF1 0.8796 | Time(s) 1.1176\n",
      "Epoch 00061 | Loss 0.1072 | Test Acc 0.8867 | mF1 0.7206 | wF1 0.8795 | Time(s) 1.1154\n",
      "Epoch 00062 | Loss 0.1054 | Test Acc 0.8867 | mF1 0.7203 | wF1 0.8796 | Time(s) 1.1135\n",
      "Epoch 00063 | Loss 0.1036 | Test Acc 0.8869 | mF1 0.7205 | wF1 0.8798 | Time(s) 1.1099\n",
      "Epoch 00064 | Loss 0.1018 | Test Acc 0.8876 | mF1 0.7214 | wF1 0.8803 | Time(s) 1.1082\n",
      "Epoch 00065 | Loss 0.1000 | Test Acc 0.8878 | mF1 0.7218 | wF1 0.8805 | Time(s) 1.1052\n",
      "Epoch 00066 | Loss 0.0982 | Test Acc 0.8878 | mF1 0.7221 | wF1 0.8804 | Time(s) 1.1040\n",
      "Epoch 00067 | Loss 0.0964 | Test Acc 0.8882 | mF1 0.7238 | wF1 0.8810 | Time(s) 1.1011\n",
      "Epoch 00068 | Loss 0.0947 | Test Acc 0.8882 | mF1 0.7256 | wF1 0.8810 | Time(s) 1.0983\n",
      "Epoch 00069 | Loss 0.0930 | Test Acc 0.8878 | mF1 0.7253 | wF1 0.8806 | Time(s) 1.0959\n",
      "Epoch 00070 | Loss 0.0913 | Test Acc 0.8884 | mF1 0.7289 | wF1 0.8814 | Time(s) 1.0943\n",
      "Epoch 00071 | Loss 0.0897 | Test Acc 0.8886 | mF1 0.7305 | wF1 0.8818 | Time(s) 1.0913\n",
      "Epoch 00072 | Loss 0.0880 | Test Acc 0.8884 | mF1 0.7303 | wF1 0.8816 | Time(s) 1.0900\n",
      "Epoch 00073 | Loss 0.0864 | Test Acc 0.8873 | mF1 0.7285 | wF1 0.8807 | Time(s) 1.0876\n",
      "Epoch 00074 | Loss 0.0848 | Test Acc 0.8884 | mF1 0.7345 | wF1 0.8822 | Time(s) 1.0857\n",
      "Epoch 00075 | Loss 0.0832 | Test Acc 0.8880 | mF1 0.7337 | wF1 0.8819 | Time(s) 1.0868\n",
      "Epoch 00076 | Loss 0.0816 | Test Acc 0.8871 | mF1 0.7325 | wF1 0.8811 | Time(s) 1.0853\n",
      "Epoch 00077 | Loss 0.0801 | Test Acc 0.8871 | mF1 0.7336 | wF1 0.8813 | Time(s) 1.0832\n",
      "Epoch 00078 | Loss 0.0785 | Test Acc 0.8867 | mF1 0.7317 | wF1 0.8808 | Time(s) 1.0815\n",
      "Epoch 00079 | Loss 0.0770 | Test Acc 0.8867 | mF1 0.7320 | wF1 0.8808 | Time(s) 1.0796\n",
      "Epoch 00080 | Loss 0.0755 | Test Acc 0.8869 | mF1 0.7345 | wF1 0.8812 | Time(s) 1.0774\n",
      "Epoch 00081 | Loss 0.0740 | Test Acc 0.8871 | mF1 0.7344 | wF1 0.8815 | Time(s) 1.0754\n",
      "Epoch 00082 | Loss 0.0726 | Test Acc 0.8863 | mF1 0.7338 | wF1 0.8806 | Time(s) 1.0729\n",
      "Epoch 00083 | Loss 0.0711 | Test Acc 0.8861 | mF1 0.7337 | wF1 0.8804 | Time(s) 1.0718\n",
      "Epoch 00084 | Loss 0.0697 | Test Acc 0.8859 | mF1 0.7322 | wF1 0.8801 | Time(s) 1.0717\n",
      "Epoch 00085 | Loss 0.0683 | Test Acc 0.8857 | mF1 0.7315 | wF1 0.8800 | Time(s) 1.0707\n",
      "Epoch 00086 | Loss 0.0669 | Test Acc 0.8857 | mF1 0.7318 | wF1 0.8800 | Time(s) 1.0724\n",
      "Epoch 00087 | Loss 0.0656 | Test Acc 0.8857 | mF1 0.7328 | wF1 0.8801 | Time(s) 1.0723\n",
      "Epoch 00088 | Loss 0.0642 | Test Acc 0.8863 | mF1 0.7346 | wF1 0.8808 | Time(s) 1.0722\n",
      "Epoch 00089 | Loss 0.0629 | Test Acc 0.8863 | mF1 0.7346 | wF1 0.8808 | Time(s) 1.0719\n",
      "Epoch 00090 | Loss 0.0616 | Test Acc 0.8865 | mF1 0.7350 | wF1 0.8810 | Time(s) 1.0720\n",
      "Epoch 00091 | Loss 0.0603 | Test Acc 0.8867 | mF1 0.7357 | wF1 0.8814 | Time(s) 1.0736\n",
      "Epoch 00092 | Loss 0.0591 | Test Acc 0.8869 | mF1 0.7358 | wF1 0.8816 | Time(s) 1.0751\n",
      "Epoch 00093 | Loss 0.0578 | Test Acc 0.8867 | mF1 0.7357 | wF1 0.8814 | Time(s) 1.0766\n",
      "Epoch 00094 | Loss 0.0566 | Test Acc 0.8867 | mF1 0.7357 | wF1 0.8814 | Time(s) 1.0784\n",
      "Epoch 00095 | Loss 0.0554 | Test Acc 0.8867 | mF1 0.7354 | wF1 0.8815 | Time(s) 1.0892\n",
      "Epoch 00096 | Loss 0.0542 | Test Acc 0.8871 | mF1 0.7370 | wF1 0.8820 | Time(s) 1.0895\n",
      "Epoch 00097 | Loss 0.0531 | Test Acc 0.8871 | mF1 0.7370 | wF1 0.8820 | Time(s) 1.0894\n",
      "Epoch 00098 | Loss 0.0519 | Test Acc 0.8873 | mF1 0.7371 | wF1 0.8822 | Time(s) 1.0893\n",
      "Epoch 00099 | Loss 0.0508 | Test Acc 0.8869 | mF1 0.7355 | wF1 0.8817 | Time(s) 1.0939\n",
      "Epoch 00100 | Loss 0.0497 | Test Acc 0.8865 | mF1 0.7363 | wF1 0.8814 | Time(s) 1.1005\n",
      "Epoch 00101 | Loss 0.0487 | Test Acc 0.8863 | mF1 0.7358 | wF1 0.8812 | Time(s) 1.1042\n",
      "Epoch 00102 | Loss 0.0476 | Test Acc 0.8859 | mF1 0.7342 | wF1 0.8807 | Time(s) 1.1067\n",
      "Epoch 00103 | Loss 0.0466 | Test Acc 0.8861 | mF1 0.7367 | wF1 0.8812 | Time(s) 1.1061\n",
      "Epoch 00104 | Loss 0.0456 | Test Acc 0.8859 | mF1 0.7366 | wF1 0.8810 | Time(s) 1.1058\n",
      "Epoch 00105 | Loss 0.0446 | Test Acc 0.8857 | mF1 0.7364 | wF1 0.8808 | Time(s) 1.1055\n",
      "Epoch 00106 | Loss 0.0437 | Test Acc 0.8857 | mF1 0.7364 | wF1 0.8808 | Time(s) 1.1069\n",
      "Epoch 00107 | Loss 0.0427 | Test Acc 0.8852 | mF1 0.7359 | wF1 0.8804 | Time(s) 1.1064\n",
      "Epoch 00108 | Loss 0.0418 | Test Acc 0.8850 | mF1 0.7357 | wF1 0.8802 | Time(s) 1.1055\n",
      "Epoch 00109 | Loss 0.0409 | Test Acc 0.8846 | mF1 0.7338 | wF1 0.8797 | Time(s) 1.1049\n",
      "Epoch 00110 | Loss 0.0400 | Test Acc 0.8848 | mF1 0.7340 | wF1 0.8799 | Time(s) 1.1030\n",
      "Epoch 00111 | Loss 0.0391 | Test Acc 0.8842 | mF1 0.7333 | wF1 0.8793 | Time(s) 1.1129\n",
      "Epoch 00112 | Loss 0.0383 | Test Acc 0.8842 | mF1 0.7330 | wF1 0.8794 | Time(s) 1.1140\n",
      "Epoch 00113 | Loss 0.0375 | Test Acc 0.8846 | mF1 0.7336 | wF1 0.8797 | Time(s) 1.1159\n",
      "Epoch 00114 | Loss 0.0367 | Test Acc 0.8844 | mF1 0.7334 | wF1 0.8795 | Time(s) 1.1222\n",
      "Epoch 00115 | Loss 0.0359 | Test Acc 0.8844 | mF1 0.7347 | wF1 0.8796 | Time(s) 1.1247\n",
      "Epoch 00116 | Loss 0.0351 | Test Acc 0.8844 | mF1 0.7347 | wF1 0.8796 | Time(s) 1.1259\n",
      "Epoch 00117 | Loss 0.0343 | Test Acc 0.8844 | mF1 0.7360 | wF1 0.8797 | Time(s) 1.1310\n",
      "Epoch 00118 | Loss 0.0336 | Test Acc 0.8842 | mF1 0.7353 | wF1 0.8796 | Time(s) 1.1315\n",
      "Epoch 00119 | Loss 0.0329 | Test Acc 0.8848 | mF1 0.7363 | wF1 0.8801 | Time(s) 1.1318\n",
      "Epoch 00120 | Loss 0.0322 | Test Acc 0.8846 | mF1 0.7349 | wF1 0.8798 | Time(s) 1.1319\n",
      "Epoch 00121 | Loss 0.0315 | Test Acc 0.8844 | mF1 0.7347 | wF1 0.8796 | Time(s) 1.1326\n",
      "Epoch 00122 | Loss 0.0309 | Test Acc 0.8840 | mF1 0.7334 | wF1 0.8791 | Time(s) 1.1332\n",
      "Epoch 00123 | Loss 0.0302 | Test Acc 0.8840 | mF1 0.7336 | wF1 0.8790 | Time(s) 1.1358\n",
      "Epoch 00124 | Loss 0.0296 | Test Acc 0.8842 | mF1 0.7341 | wF1 0.8792 | Time(s) 1.1348\n",
      "Epoch 00125 | Loss 0.0290 | Test Acc 0.8840 | mF1 0.7336 | wF1 0.8790 | Time(s) 1.1341\n",
      "Epoch 00126 | Loss 0.0284 | Test Acc 0.8842 | mF1 0.7338 | wF1 0.8793 | Time(s) 1.1347\n",
      "Epoch 00127 | Loss 0.0278 | Test Acc 0.8840 | mF1 0.7336 | wF1 0.8790 | Time(s) 1.1342\n",
      "Epoch 00128 | Loss 0.0272 | Test Acc 0.8840 | mF1 0.7347 | wF1 0.8792 | Time(s) 1.1378\n",
      "Epoch 00129 | Loss 0.0267 | Test Acc 0.8838 | mF1 0.7343 | wF1 0.8790 | Time(s) 1.1371\n",
      "Epoch 00130 | Loss 0.0261 | Test Acc 0.8840 | mF1 0.7344 | wF1 0.8792 | Time(s) 1.1368\n",
      "Epoch 00131 | Loss 0.0256 | Test Acc 0.8840 | mF1 0.7347 | wF1 0.8792 | Time(s) 1.1363\n",
      "Epoch 00132 | Loss 0.0251 | Test Acc 0.8842 | mF1 0.7345 | wF1 0.8794 | Time(s) 1.1366\n",
      "Epoch 00133 | Loss 0.0246 | Test Acc 0.8844 | mF1 0.7360 | wF1 0.8797 | Time(s) 1.1399\n",
      "Epoch 00134 | Loss 0.0241 | Test Acc 0.8844 | mF1 0.7357 | wF1 0.8798 | Time(s) 1.1406\n",
      "Epoch 00135 | Loss 0.0236 | Test Acc 0.8842 | mF1 0.7356 | wF1 0.8796 | Time(s) 1.1413\n",
      "Epoch 00136 | Loss 0.0232 | Test Acc 0.8840 | mF1 0.7354 | wF1 0.8793 | Time(s) 1.1429\n",
      "Epoch 00137 | Loss 0.0227 | Test Acc 0.8842 | mF1 0.7366 | wF1 0.8797 | Time(s) 1.1445\n",
      "Epoch 00138 | Loss 0.0223 | Test Acc 0.8844 | mF1 0.7367 | wF1 0.8799 | Time(s) 1.1449\n",
      "Epoch 00139 | Loss 0.0219 | Test Acc 0.8844 | mF1 0.7364 | wF1 0.8799 | Time(s) 1.1440\n",
      "Epoch 00140 | Loss 0.0215 | Test Acc 0.8842 | mF1 0.7363 | wF1 0.8797 | Time(s) 1.1432\n",
      "Epoch 00141 | Loss 0.0211 | Test Acc 0.8842 | mF1 0.7366 | wF1 0.8797 | Time(s) 1.1440\n",
      "Epoch 00142 | Loss 0.0207 | Test Acc 0.8842 | mF1 0.7366 | wF1 0.8797 | Time(s) 1.1482\n",
      "Epoch 00143 | Loss 0.0203 | Test Acc 0.8844 | mF1 0.7367 | wF1 0.8799 | Time(s) 1.1539\n",
      "Epoch 00144 | Loss 0.0199 | Test Acc 0.8846 | mF1 0.7369 | wF1 0.8801 | Time(s) 1.1573\n",
      "Epoch 00145 | Loss 0.0195 | Test Acc 0.8844 | mF1 0.7354 | wF1 0.8798 | Time(s) 1.1569\n",
      "Epoch 00146 | Loss 0.0192 | Test Acc 0.8846 | mF1 0.7359 | wF1 0.8800 | Time(s) 1.1566\n",
      "Epoch 00147 | Loss 0.0188 | Test Acc 0.8844 | mF1 0.7360 | wF1 0.8797 | Time(s) 1.1561\n",
      "Epoch 00148 | Loss 0.0185 | Test Acc 0.8848 | mF1 0.7363 | wF1 0.8801 | Time(s) 1.1554\n",
      "Epoch 00149 | Loss 0.0182 | Test Acc 0.8848 | mF1 0.7363 | wF1 0.8801 | Time(s) 1.1557\n",
      "Epoch 00150 | Loss 0.0178 | Test Acc 0.8850 | mF1 0.7364 | wF1 0.8804 | Time(s) 1.1546\n",
      "Epoch 00151 | Loss 0.0175 | Test Acc 0.8852 | mF1 0.7368 | wF1 0.8805 | Time(s) 1.1567\n",
      "Epoch 00152 | Loss 0.0172 | Test Acc 0.8850 | mF1 0.7367 | wF1 0.8803 | Time(s) 1.1561\n",
      "Epoch 00153 | Loss 0.0169 | Test Acc 0.8854 | mF1 0.7383 | wF1 0.8808 | Time(s) 1.1560\n",
      "Epoch 00154 | Loss 0.0166 | Test Acc 0.8852 | mF1 0.7381 | wF1 0.8806 | Time(s) 1.1564\n",
      "Epoch 00155 | Loss 0.0163 | Test Acc 0.8854 | mF1 0.7386 | wF1 0.8808 | Time(s) 1.1556\n",
      "Epoch 00156 | Loss 0.0161 | Test Acc 0.8852 | mF1 0.7384 | wF1 0.8806 | Time(s) 1.1565\n",
      "Epoch 00157 | Loss 0.0158 | Test Acc 0.8854 | mF1 0.7383 | wF1 0.8808 | Time(s) 1.1567\n",
      "Epoch 00158 | Loss 0.0155 | Test Acc 0.8852 | mF1 0.7379 | wF1 0.8807 | Time(s) 1.1564\n",
      "Epoch 00159 | Loss 0.0153 | Test Acc 0.8852 | mF1 0.7376 | wF1 0.8807 | Time(s) 1.1553\n",
      "Epoch 00160 | Loss 0.0150 | Test Acc 0.8852 | mF1 0.7376 | wF1 0.8807 | Time(s) 1.1540\n",
      "Epoch 00161 | Loss 0.0148 | Test Acc 0.8852 | mF1 0.7376 | wF1 0.8807 | Time(s) 1.1531\n",
      "Epoch 00162 | Loss 0.0145 | Test Acc 0.8852 | mF1 0.7376 | wF1 0.8807 | Time(s) 1.1564\n",
      "Epoch 00163 | Loss 0.0143 | Test Acc 0.8857 | mF1 0.7382 | wF1 0.8811 | Time(s) 1.1588\n",
      "Epoch 00164 | Loss 0.0140 | Test Acc 0.8857 | mF1 0.7382 | wF1 0.8811 | Time(s) 1.1584\n",
      "Epoch 00165 | Loss 0.0138 | Test Acc 0.8857 | mF1 0.7382 | wF1 0.8811 | Time(s) 1.1583\n",
      "Epoch 00166 | Loss 0.0136 | Test Acc 0.8852 | mF1 0.7376 | wF1 0.8807 | Time(s) 1.1575\n",
      "Epoch 00167 | Loss 0.0134 | Test Acc 0.8852 | mF1 0.7376 | wF1 0.8807 | Time(s) 1.1563\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 17\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# backward propagation\u001B[39;00m\n\u001B[0;32m     16\u001B[0m opt\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 17\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m opt\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m epoch \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m3\u001B[39m:\n",
      "File \u001B[1;32mD:\\nguyennha\\TextGraph\\.venv\\Lib\\site-packages\\torch\\_tensor.py:525\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    515\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    517\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    518\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    523\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    524\u001B[0m     )\n\u001B[1;32m--> 525\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    526\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    527\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\nguyennha\\TextGraph\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    262\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    264\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    265\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    266\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 267\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    274\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    275\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\nguyennha\\TextGraph\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    742\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    743\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 744\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    745\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    746\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    747\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    748\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "File \u001B[1;32mD:\\nguyennha\\TextGraph\\.venv\\Lib\\site-packages\\torch\\autograd\\function.py:301\u001B[0m, in \u001B[0;36mBackwardCFunction.apply\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m    295\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    296\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mImplementing both \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbackward\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m and \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvjp\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m for a custom \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    297\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFunction is not allowed. You should only implement one \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    298\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mof them.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    299\u001B[0m     )\n\u001B[0;32m    300\u001B[0m user_fn \u001B[38;5;241m=\u001B[39m vjp_fn \u001B[38;5;28;01mif\u001B[39;00m vjp_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m Function\u001B[38;5;241m.\u001B[39mvjp \u001B[38;5;28;01melse\u001B[39;00m backward_fn\n\u001B[1;32m--> 301\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43muser_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\nguyennha\\TextGraph\\.venv\\Lib\\site-packages\\dgl\\backend\\pytorch\\sparse.py:215\u001B[0m, in \u001B[0;36mGSpMM.backward\u001B[1;34m(ctx, dZ)\u001B[0m\n\u001B[0;32m    213\u001B[0m         dX \u001B[38;5;241m=\u001B[39m gspmm(g_rev, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcopy_lhs\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msum\u001B[39m\u001B[38;5;124m\"\u001B[39m, dZ, Y)\n\u001B[0;32m    214\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m op \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcopy_lhs\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 215\u001B[0m         dX \u001B[38;5;241m=\u001B[39m \u001B[43mgspmm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mg_rev\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcopy_lhs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msum\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdZ\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    216\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# max/min\u001B[39;00m\n\u001B[0;32m    217\u001B[0m     dX \u001B[38;5;241m=\u001B[39m th\u001B[38;5;241m.\u001B[39mzeros(\n\u001B[0;32m    218\u001B[0m         (X_shape[\u001B[38;5;241m0\u001B[39m],) \u001B[38;5;241m+\u001B[39m dZ\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m:], dtype\u001B[38;5;241m=\u001B[39mdtype, device\u001B[38;5;241m=\u001B[39mdevice\n\u001B[0;32m    219\u001B[0m     )\n",
      "File \u001B[1;32mD:\\nguyennha\\TextGraph\\.venv\\Lib\\site-packages\\dgl\\backend\\pytorch\\sparse.py:1032\u001B[0m, in \u001B[0;36mgspmm\u001B[1;34m(gidx, op, reduce_op, lhs_data, rhs_data)\u001B[0m\n\u001B[0;32m   1030\u001B[0m args \u001B[38;5;241m=\u001B[39m _cast_if_autocast_enabled(gidx, op, reduce_op, lhs_data, rhs_data)\n\u001B[0;32m   1031\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _disable_autocast_if_enabled():\n\u001B[1;32m-> 1032\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mGSpMM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\nguyennha\\TextGraph\\.venv\\Lib\\site-packages\\torch\\autograd\\function.py:598\u001B[0m, in \u001B[0;36mFunction.apply\u001B[1;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[0;32m    595\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_are_functorch_transforms_active():\n\u001B[0;32m    596\u001B[0m     \u001B[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001B[39;00m\n\u001B[0;32m    597\u001B[0m     args \u001B[38;5;241m=\u001B[39m _functorch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39munwrap_dead_wrappers(args)\n\u001B[1;32m--> 598\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m    600\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_setup_ctx_defined:\n\u001B[0;32m    601\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    602\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIn order to use an autograd.Function with functorch transforms \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    603\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    604\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstaticmethod. For more details, please see \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    605\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    606\u001B[0m     )\n",
      "File \u001B[1;32mD:\\nguyennha\\TextGraph\\.venv\\Lib\\site-packages\\dgl\\backend\\pytorch\\sparse.py:165\u001B[0m, in \u001B[0;36mGSpMM.forward\u001B[1;34m(ctx, gidx, op, reduce_op, X, Y)\u001B[0m\n\u001B[0;32m    163\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[0;32m    164\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(ctx, gidx, op, reduce_op, X, Y):\n\u001B[1;32m--> 165\u001B[0m     out, (argX, argY) \u001B[38;5;241m=\u001B[39m \u001B[43m_gspmm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgidx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduce_op\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    166\u001B[0m     reduce_last \u001B[38;5;241m=\u001B[39m _need_reduce_last_dim(X, Y)\n\u001B[0;32m    167\u001B[0m     X_shape \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;28;01mif\u001B[39;00m X \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mD:\\nguyennha\\TextGraph\\.venv\\Lib\\site-packages\\dgl\\_sparse_ops.py:239\u001B[0m, in \u001B[0;36m_gspmm\u001B[1;34m(gidx, op, reduce_op, u, e)\u001B[0m\n\u001B[0;32m    237\u001B[0m arg_e_nd \u001B[38;5;241m=\u001B[39m to_dgl_nd_for_write(arg_e)\n\u001B[0;32m    238\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m gidx\u001B[38;5;241m.\u001B[39mnum_edges(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m--> 239\u001B[0m     \u001B[43m_CAPI_DGLKernelSpMM\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    240\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgidx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    241\u001B[0m \u001B[43m        \u001B[49m\u001B[43mop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    242\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreduce_op\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    243\u001B[0m \u001B[43m        \u001B[49m\u001B[43mto_dgl_nd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mu\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43muse_u\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    244\u001B[0m \u001B[43m        \u001B[49m\u001B[43mto_dgl_nd\u001B[49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43muse_e\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    245\u001B[0m \u001B[43m        \u001B[49m\u001B[43mto_dgl_nd_for_write\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    246\u001B[0m \u001B[43m        \u001B[49m\u001B[43marg_u_nd\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    247\u001B[0m \u001B[43m        \u001B[49m\u001B[43marg_e_nd\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    248\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    249\u001B[0m \u001B[38;5;66;03m# NOTE(zihao): actually we can avoid the following step, because arg_*_nd\u001B[39;00m\n\u001B[0;32m    250\u001B[0m \u001B[38;5;66;03m# refers to the data that stores arg_*. After we call _CAPI_DGLKernelSpMM,\u001B[39;00m\n\u001B[0;32m    251\u001B[0m \u001B[38;5;66;03m# arg_* should have already been changed. But we found this doesn't work\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    254\u001B[0m \u001B[38;5;66;03m# The workaround is proposed by Jinjing, and we still need to investigate\u001B[39;00m\n\u001B[0;32m    255\u001B[0m \u001B[38;5;66;03m# where the problem is.\u001B[39;00m\n\u001B[0;32m    256\u001B[0m arg_u \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m arg_u \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m F\u001B[38;5;241m.\u001B[39mzerocopy_from_dgl_ndarray(arg_u_nd)\n",
      "File \u001B[1;32mD:\\nguyennha\\TextGraph\\.venv\\Lib\\site-packages\\dgl\\_ffi\\_ctypes\\function.py:213\u001B[0m, in \u001B[0;36mFunctionBase.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m    210\u001B[0m ret_val \u001B[38;5;241m=\u001B[39m DGLValue()\n\u001B[0;32m    211\u001B[0m ret_tcode \u001B[38;5;241m=\u001B[39m ctypes\u001B[38;5;241m.\u001B[39mc_int()\n\u001B[0;32m    212\u001B[0m check_call(\n\u001B[1;32m--> 213\u001B[0m     \u001B[43m_LIB\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDGLFuncCall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    214\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    215\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    216\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtcodes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    217\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctypes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mc_int\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_args\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    218\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctypes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbyref\u001B[49m\u001B[43m(\u001B[49m\u001B[43mret_val\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    219\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctypes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbyref\u001B[49m\u001B[43m(\u001B[49m\u001B[43mret_tcode\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    220\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    221\u001B[0m )\n\u001B[0;32m    222\u001B[0m _ \u001B[38;5;241m=\u001B[39m temp_args\n\u001B[0;32m    223\u001B[0m _ \u001B[38;5;241m=\u001B[39m args\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T17:13:38.998620Z",
     "start_time": "2025-03-28T17:13:38.981719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Max accuracy: {max_acc:.4f}\")\n",
    "print(f'Max F1: {max_f1:.4f}')"
   ],
   "id": "e25afb12e9f137c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max accuracy: 0.8886\n",
      "Max F1: 0.8822\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b67f521b939d24d5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
